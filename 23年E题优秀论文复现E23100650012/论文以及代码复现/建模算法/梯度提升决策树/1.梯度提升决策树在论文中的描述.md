# 梯度提升决策树在论文中的描述

## 1. 算法概述

**4. 梯度提升决策树:**是 Boosting 算法的一种改进。该模型的核心在于利用损失函数的负梯度在当前模型的值作为残差的近似值,对损失函数进行泰勒展开,拟合回归树。

梯度提升决策树的学习算法可以描述为:算法在每次迭代中生成一颗新的决策树,在下次迭代开始之前计算损失函数,以及损失函数在每个样本上的一阶、二阶导数,随后通过贪心策略生成新的决策树,并且计算每个节点所对应的预测值,把新生成的决策树添加到模型中。需要特别注意的是,梯度提升决策树中每次模型建立都是为了使先前模型残差向着梯度方向减少。**与随机森林的原理类似,在本问中,对多个基学习器的结果进行整合,**以获得最终的分类结果和相应的置信度预测。

## 2. 在分类任务中的应用

常见的分类模型有 k 近邻算法,支持向量机分类器,随机森林分类器和梯度提升决策树。本文通过对比这四种模型,利用 K 则交叉验证方法选择出其中最优的模型。

构建了**基于分类任务的概率预测模型**,分别对比了k近邻算法,支持向量机分类器,随机森林分类器和梯度提升决策树,利用每个模型输出的置信度作为最终的概率预测。经过对比,梯度提升分类器获得了最佳的效果,获得了0.0524 的 MAE 数值。

## 3. 模型选择的原因

**模型的选择:**前四组实验分别对比四种模型的性能。为了公平直观地进行对比,并没有对四种方法进行变量筛选等操作。从评价指标来看,梯度提升机获得了最佳的表现。在本问中,发生血肿扩张的人数为 23 人,未发生血肿扩张的为 77 人,存在着一定情况的类别不平衡问题,同时,经过对数据的审视,发现原始数据也存在着一定量的离散变量。而梯度提升机在处理类别偏差和混合数据类型具有一定的优势,故最终选择梯度提升机作为第一问的概率预测基础模型。

## 4. 梯度提升回归的描述

### 3. 梯度提升回归

梯度提升回归(GBRT)是 Boosting 算法的一种改进,其核心思想是利用损失函数的负梯度在当前模型的值作为残差的近似值,本质上是对损失函数进行一阶泰勒展开,从而拟合一个回归树。梯度提升回归树由多棵回归树和梯度提升组成。所有回归树的结论累加起来为最终结果,且每一棵回归树都是学习之前的所有回归树的结论和残差。

## 5. 在对比实验中的表现

为了验证 Stacking 集成算法的有效性,我们比较问题一中所使用的分类模型,包括有 k 近邻算法,支持向量机分类器,随机森林分类器和梯度提升决策树。为了公平对比,所有模型都使用相同的变量,相同的数据增强手段和同样次数的 K 则交叉验证。

### 分类模型指标对比结果

| 分类模型     | 精确率   | 召回率   |
|----------|-------|-------|
| 梯度提升决策树  | 0.781 | 0.731 |

### 加入随访数据后的对比结果

|         | 分类模型     | 精确率   | 召回率   |
|---------|----------|-------|-------|
| 未加入随访数据 | 梯度提升决策树  | 0.781 | 0.731 |
| 加入随访数据  | 梯度提升决策树  | 0.868 | 0.838 |

## 6. 在回归任务中的表现

为了探究最适合于本问的拟合模型,分别选取了线性拟合和非线性拟合的方法进行对比,具体有线性回归、随机森林、梯度提升回归和决策树回归四种模型。区别于第一问的分类模型,本问中使用这些模型进行回归拟合。

| 模型     | MAE        | RMSE       | $R^2$  |
|--------|------------|------------|--------|
| 梯度提升回归 | 13465.3358 | 21747.4794 | 0.5450 |

为了更直观地对比各个模型的预测结果,我们将全部患者的检查时间作为横坐标,对应水肿体积预测值与真实值的残差作为纵坐标,画出了各个模型的残差图,如下图所示,可以发现,线性模型、随机森林以及梯度提升回归模型都对训练集欠拟合,模型的性能不如梯度提升模型。

分别对以上三个亚组进行拟合模型的构建,为了便于对比,模型和变量选择同问题 a。在进行线性回归、随机森林模型、梯度提升回归、决策树回归模型拟合后,发现决策树回归模型的效果最好,拟合曲线如图 12,且亚组拟合模型评价结果如表 11。 