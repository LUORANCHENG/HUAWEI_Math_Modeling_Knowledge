# 为什么选用KNN算法

## 1. 选择KNN的背景和原因

### 1.1 问题背景
作者在论文中面临的是**出血性脑卒中患者血肿扩张概率预测**的问题。经过实验，作者发现直接采用回归模型进行预测，难以达到收敛的效果。因此，作者选择使用分类模型对是否发生血肿扩张进行分类预测，利用分类结果的置信度作为血肿扩张的概率值。

### 1.2 模型对比选择的需要
作者需要从常见的分类模型中选择最优的模型，这些模型包括：
- k近邻算法
- 支持向量机分类器
- 随机森林分类器
- 梯度提升决策树

通过对比这四种模型，利用K则交叉验证方法选择出其中最优的模型。

## 2. KNN算法的选择原因

### 2.1 算法特性适合该问题
K近邻算法具有以下特点，使其适合作为对比模型：

1. **基本且经典**：是一种基本的分类与回归算法，核心功能是解决有监督的分类问题
2. **无参数学习**：属于非参数学习算法，不需要学习任何参数
3. **惰性学习**：不具有显式的学习过程，属于"惰性学习"的一种
4. **直观易理解**：核心思想是通过测量不同特征值之间的距离进行分类

### 2.2 能够输出置信度
在本问题中，需要输出分类结果的置信度，KNN算法可以通过以下方式实现：
- 使用次数出现最多的类别数目除以k
- 样本的置信度为：$p_c = (\max_c \sum_{\mathbf{x}_i \in \mathcal{N}_K(\mathbf{x})} I(y_i = c))/k$

## 3. KNN算法的具体用途

### 3.1 作为基线对比模型
KNN算法主要被用作**基线对比模型**，用于：
- 验证其他更复杂算法的有效性
- 提供性能对比的基准
- 确保模型选择的公平性和科学性

### 3.2 在不同实验中的应用

#### 3.2.1 消融实验中的基线
在消融实验中，KNN作为实验组A的基线模型：
- MAE: 0.3107
- MSE: 0.1606
- R²: 0.1344

#### 3.2.2 在问题三中的对比验证
为了验证Stacking集成算法的有效性，KNN被用作对比方法：
- 问题三-a：精确率0.652，召回率0.568
- 问题三-b：未加入随访数据时精确率0.652，召回率0.568；加入随访数据后精确率0.692，召回率0.613

### 3.3 特征权重函数选择的工具
KNN还被用于**特征权重函数的选择**：
- 利用K则交叉验证+KNN的方法对函数和参数K进行选择
- 最终确定当使用幂函数形式且参数k选择0.7时，KNN模型获得了最佳的表现
- 这个结论被用于指导后续梯度提升机模型中权重函数的设计

## 4. KNN选择的科学性考虑

### 4.1 公平对比的保证
作者选择KNN的一个重要原因是确保**公平对比**：
- 所有模型都使用相同的变量
- 相同的数据增强手段
- 同样次数的K则交叉验证
- 为了公平直观地进行对比，并没有对四种方法进行变量筛选等操作

### 4.2 算法多样性的需要
KNN作为一种**基于距离的非参数算法**，与其他算法形成了良好的对比：
- SVM：基于间隔最大化的算法
- 随机森林：基于决策树集成的算法
- 梯度提升：基于boosting的算法
- KNN：基于距离的算法

这种多样性确保了模型选择的全面性和可靠性。

## 5. 最终选择结论

虽然KNN算法被包含在对比实验中，但最终作者选择了梯度提升机作为主要模型，原因是：
- 梯度提升机获得了最佳的表现
- 在类别不平衡问题（发生血肿扩张23人，未发生77人）中表现更好
- 在处理混合数据类型方面具有优势
- 对类别偏差和混合数据类型具有一定的优势

**总结：KNN算法的选择主要是为了提供科学、公平的基线对比，验证其他算法的有效性，并在特征工程中发挥辅助作用，而非作为最终的预测模型。** 