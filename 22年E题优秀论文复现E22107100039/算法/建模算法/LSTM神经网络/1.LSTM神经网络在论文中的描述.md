# LSTM神经网络在论文中的描述

## 1. 摘要中的描述

**对未来不同深度的土壤湿度的预测(问题二)**,本题结合给定的土壤湿度数据、 土壤蒸发数据和降水等数据变量,并选取相应的**时间序列预测**回归模型进行求解。首 先假设保持目前放牧策略不变,不同深度土壤湿度具有时序特征,使用**滑动窗口**的原 理进行时间序列划分,建立监督学习回归分析的数据,尝试建立 **BP 神经网络**模型, 但将题目所给数据代入后得出全连接的神经网络模型在已给数据中划分的测试集上 的拟合度较低、误差较大;为提高模型预测准确度,本题又建立了**长短时记忆网络** (LSTM)模型与带有注意力机制的双向长短时记忆网络(Attention-BiLSTM)模型 预测 2022 年、2023 年不同深度的土壤湿度。经过超参数优化和模型对比后得出 Attention-BiLSTM 模型在测试集的 R<sup>2</sup>、MSE、MAE、MAPE 各项指标都明显好于 其他神经网络,因此本题采用 R<sup>2</sup>为 0.88 的 Attention-BiLSTM 模型,得出 2022 年、 2023 年不同深度的结论。

## 2. LSTM神经网络详细介绍

### 2.1 LSTM神经网络概述

LSTM(Long Short-Term Memory,长短时记忆)神经网络是循环神经网络[13](Recurrent Neural Network, RNN)的一种,可以从时间序列中学习长期依赖关系,广泛应用于深度 学习。LSTM 神经网络专为输入数据按时间序列排序的实际应用程序而设计。其中,序列 中要预测的值前几个时刻的信息对于训练效果是必不可少的。简单的 RNN 无法处理长距 离依赖,其隐藏层只有一个状态 h,对短期输入非常敏感。LSTM 在 RNN 中加入了一个状态 c,称为 cell state,这样就可以保存长期的状态。

LSTM 中的节点使用内部状态作为数据内存存储单元。它们可以在多个时间步长上存储和检索信息。输入值、先前的输出和内部状态都用于节点计算。计算结果不仅用于提供输出值,还用于更新状态。LSTM 节点与普通神经网络节点一样,具有确定计算中使用的输入数据权重的参数,但 LSTM 也具有称为门的参数,用于控制节点内的信息流。这些门的参数包括训练产生的权重和偏差。

LSTM 神经网络的结构如图 4-17 所示:

### 2.2 LSTM数学公式

在时间*t*, *h*<sub>t</sub>代表短期记忆, *c*<sub>t</sub>代表长期记忆。遗忘门的作用是控制网络下次更新状态时需要丢弃的信息量。遗忘门的状态更新公式为:

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

输入门的作用是控制和选择神经网络的输入数据的哪一部分应该保留在单元中。输入 门的状态更新公式为:

$$i_{t} = \sigma \left( W_{i} \cdot [h_{t-1}, x_{t}] + b_{i} \right)$$
$$\tilde{c}_{t} = \tanh(W_{c} \cdot [h_{t-1}, x_{t}] + b_{c})$$
$$c_{t} = f_{t} * c_{t-1} + i_{t} * \tilde{c}_{t}$$

输出门的作用是从神经网络的当前单元状态中控制和选择要输出的数据。输出门的状态更新公式为:

$$o_{t} = \sigma \left( W_{o} \left[ h_{t-1}, x_{t} \right] + b_{o} \right)$$
$$h_{t} = o_{t} * \tanh(c_{t})$$

上述等式和图中符号的含义如下:

$f_t$ :遗忘门在时间t的激活向量

$i_t$ : 输入门在时间t的激活向量

$o_t$ : 输出门在时间t的激活向量

$x_t$ : LSTM 单元在时间t的输入向量

$y_t$ : LSTM 单元在时间t的输出向量

$c_t$ : *t*的 cell 状态信息

$\tilde{c}_t$ : *t*时刻的 cell 输入激活向量

W:更新状态时 LSTM 单元的权重

b: 计算过程中的偏置项

$h_t$ : LSTM 单元在时间t的隐藏状态向量

LSTM 门的值从 0 到 1 不等,代表允许通过多少信息,所以使用 Sigmoid 函数 ( $\sigma$ ) 作为激活函数。

## 3. 双向LSTM网络

双向长短时记忆(BiLSTM, Bidirectional LSTM)网络[14]是使 LSTM 神经网络在向后(从未来到过去)或向前(从过去到未来)两个方向上都具有序列信息的模型。

在双向 LSTM 网络中,输入在两个方向上流动,使得 BiLSTM 与常规 LSTM 不同。使

用常规LSTM,输入流指向一个方向,向后或向前。但是,在双向LSTM 网络中,可以使输入双向流动,以保留未来和过去的信息。

BiLSTM 网络拥有来自后向层和前向层的信息流。BiLSTM 通常用于需要序列到序列 任务的地方。这种网络可用于文本分类、语音识别和预测模型。

## 4. 模型训练

BP 神经网络模型、LSTM 模型和 Attention-BiLSTM 模型训练的流程图如图 4-18 所示。 首先,对时间序列数据进行预处理,以构建监督学习数据集。然后,为模型选择一组超参数。之后,将偏移的土壤物理性质数据输入模型。

对于每一轮 epoch 的训练,将训练集划分 80%用于模型训练,将剩下的 20%作为验证 集(validation set),计算并记录所构建模型这个 epoch 在验证集上的 loss 和评价指标。如 果输出的 loss 值能够满足我们的期望,模型将被保留。否则,重复上述过程,直到模型能 够满足我们准确预测未来土壤物理性质的需求。最后,保存具有最优超参数选择的模型。

由于求未来的草原土壤物理性质是一个回归问题,使用 ReLU 作为神经网络隐藏层的 激活函数,并将 tanh 设置为 LSTM 的激活函数。

$$ReLU(x) = \begin{cases} 0, & x \le 0\\ x, & x > 0 \end{cases}$$
$$tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$

在训练的每个 epoch 中,随机选择训练集的 80%数据输入网络,剩余的 20%用于验证。 平均绝对误差 (Mean Absolute Error, MAE) 用作神经网络的损失函数,均方误差 (Mean Squared Error, MSE) 用于验证每个 epoch 训练性能的指标。在每个 epoch 之后,数据集都 会被打乱。在训练过程中,早停策略(Early Stopping)用于控制训练 epoch 的数量,以防 止模型过拟合。如果验证集上的 MSE 连续 10 个 epoch 没有下降,则认为模型在当前超参数组合下达到了最优训练效果。

输入模型的表的特征数量由滑动窗口的长度 n 决定,这意味着调整 n 会影响模型的训练效果。因此,除了改变模型的超参数外,还需要仔细选择和调整之前预处理阶段的滑动窗口大小,以使模型发挥最佳精度。

模型中的超参数,包括批(batch)大小、全连接层的层数及其神经元数量和 LSTM 层的大小被不断调整,使模型在验证数据上的性能达到预期。

另外,dropout 方法也用于防止神经网络过拟合。通过在每个训练批次中忽略一定数量的特征,可以显着减少过拟合(overfitting)。这种方法可以减少隐藏层节点之间的交互, 使模型更具泛化性。

## 5. LSTM模型参数和对比结果

### 5.1 LSTM网络的最佳超参数和最佳滑动窗口长度

| 名称          | 取值   |
|-------------|------|
| 隐藏层数        | 2    |
| 隐藏层1神经元数    | 64   |
| 隐藏层 2 神经元数  | 32   |
| LSTM 层 size | 64   |
| Batch size  | 32   |
| Epoch       | 105  |
| 优化器         | Adam |
| 滑动窗口长度      | 48   |

### 5.2 模型对比结果

3种模型在测试集上的 R<sup>2</sup>、MSE、RMSE 和 MAPE 如下表所示：

| 模型               | R2    | MSE    | RMSE  | MAPE   |
|------------------|-------|--------|-------|--------|
| BP 神经网络          | 0.477 | 17.581 | 4.193 | 7.939% |
| LSTM             | 0.556 | 15.729 | 3.966 | 7.289% |
| Attention-BiLSTM | 0.716 | 10.074 | 3.174 | 5.540% |

可以看出,Attention-BiLSTM 在测试集上的4种评价指标的表现都是最好的。这是因为,双向LSTM 既可以捕捉正向的时间序列关系,也可以捕捉反向的时间序列关系;注意力机制可以让神经网络注意到哪些特征更加重要,从而为它们赋予更高的权重,提升预测的准确程度。

Attention-BiLSTM 模型训练过程中 loss 下降趋势图如图 4-21 所示。在第 144 个周期, 早停(Early Stopping)策略发挥作用,验证集的 loss 不再下降,此刻停止训练,防止了过 拟合的发生。 