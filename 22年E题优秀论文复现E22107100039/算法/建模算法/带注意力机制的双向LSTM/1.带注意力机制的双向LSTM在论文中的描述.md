# 带注意力机制的双向LSTM在论文中的描述

## 摘要中的提及

**对未来不同深度的土壤湿度的预测(问题二)**,本题结合给定的土壤湿度数据、 土壤蒸发数据和降水等数据变量,并选取相应的**时间序列预测**回归模型进行求解。首 先假设保持目前放牧策略不变,不同深度土壤湿度具有时序特征,使用**滑动窗口**的原 理进行时间序列划分,建立监督学习回归分析的数据,尝试建立 **BP 神经网络**模型, 但将题目所给数据代入后得出全连接的神经网络模型在已给数据中划分的测试集上 的拟合度较低、误差较大;为提高模型预测准确度,本题又建立了**长短时记忆网络** (LSTM)模型与带有注意力机制的双向长短时记忆网络(Attention-BiLSTM)模型 预测 2022 年、2023 年不同深度的土壤湿度。经过超参数优化和模型对比后得出 Attention-BiLSTM 模型在测试集的 R²、MSE、MAE、MAPE 各项指标都明显好于 其他神经网络,因此本题采用 R²为 0.88 的 Attention-BiLSTM 模型,得出 2022 年、 2023 年不同深度的结论。

## 4.5 带有注意力机制的双向 LSTM 神经网络

### 4.5.1 注意力机制

在神经网络中,注意力机制是一种模仿人类认知事物的注意力的技术。这种效果增强 了输入数据的某些部分,同时减少了其他部分。注意力机制的动机是网络应该更多地关注 数据的小但重要的部分。学习哪一部分数据比另一部分更重要取决于上下文,这一行为通 过梯度下降训练。

神经网络被认为是一种计算机模拟大脑神经元动作的模型,注意力机制也是一种尝试 实现相同的动作,即选择性地专注于一些相关的事情,而忽略深度神经网络中的其他事情。 神经网络的注意力层接受三个输入:查询、值和键。这些输入通常是相同的,其中查询是 一个键并且键和值是相等的。

### 4.5.2 双向 LSTM 网络

双向长短时记忆(BiLSTM, Bidirectional LSTM)网络[14]是使 LSTM 神经网络在向后(从未来到过去)或向前(从过去到未来)两个方向上都具有序列信息的模型。

在双向 LSTM 网络中,输入在两个方向上流动,使得 BiLSTM 与常规 LSTM 不同。使用常规LSTM,输入流指向一个方向,向后或向前。但是,在双向LSTM 网络中,可以使输入双向流动,以保留未来和过去的信息。

BiLSTM 网络拥有来自后向层和前向层的信息流。BiLSTM 通常用于需要序列到序列 任务的地方。这种网络可用于文本分类、语音识别和预测模型。

## 4.6 模型训练

BP 神经网络模型、LSTM 模型和 Attention-BiLSTM 模型训练的流程图如图 4-18 所示。 首先,对时间序列数据进行预处理,以构建监督学习数据集。然后,为模型选择一组超参数。之后,将偏移的土壤物理性质数据输入模型。

对于每一轮 epoch 的训练,将训练集划分 80%用于模型训练,将剩下的 20%作为验证 集(validation set),计算并记录所构建模型这个 epoch 在验证集上的 loss 和评价指标。如 果输出的 loss 值能够满足我们的期望,模型将被保留。否则,重复上述过程,直到模型能 够满足我们准确预测未来土壤物理性质的需求。最后,保存具有最优超参数选择的模型。

## Attention-BiLSTM 网络的最佳超参数和最佳滑动窗口长度

表 4-6 Attention-BiLSTM 网络的最佳超参数和最佳滑动窗口长度

| 名称          | 取值   |
|-------------|------|
| 隐藏层数        | 2    |
| 隐藏层1神经元数    | 64   |
| 隐藏层 2 神经元数  | 16   |
| LSTM 层 size | 64   |
| Batch size  | 64   |
| Epoch       | 144  |
| 优化器         | Adam |
| 滑动窗口长度      | 72   |

## 4.8 模型对比

这3种模型在测试集上的 R²、MSE、RMSE 和 MAPE 如表 4-7 所示。

表 4-7 3 种模型在测试集的模型评价

| 模型               | R2    | MSE    | RMSE  | MAPE   |
|------------------|-------|--------|-------|--------|
| BP 神经网络          | 0.477 | 17.581 | 4.193 | 7.939% |
| LSTM             | 0.556 | 15.729 | 3.966 | 7.289% |
| Attention-BiLSTM | 0.716 | 10.074 | 3.174 | 5.540% |

可以看出,Attention-BiLSTM 在测试集上的4种评价指标的表现都是最好的。这是因为,双向LSTM 既可以捕捉正向的时间序列关系,也可以捕捉反向的时间序列关系;注意力机制可以让神经网络注意到哪些特征更加重要,从而为它们赋予更高的权重,提升预测的准确程度。

Attention-BiLSTM 模型训练过程中 loss 下降趋势图如图 4-21 所示。在第 144 个周期, 早停(Early Stopping)策略发挥作用,验证集的 loss 不再下降,此刻停止训练,防止了过 拟合的发生。

接下来,分析 Attention-BiLSTM 模型在测试集上的表现。由于数据量较大,可视化所 有数据的图像无法清楚显示,所以取 15 组测试集的数据,每组数据包括 4 项,分别为任务要预测的 10cm 湿度(kg/m2)、40cm 湿度(kg/m2)、100cm 湿度(kg/m2)和 200cm 湿度(kg/m2) 这 4 项特征。

折线图对比图中,蓝色折线代表真实值,橙色折线代表预测值。真实值和预测值的折 线图对比图如图 4-22 所示。

## 代码实现片段

在论文的代码部分，可以看到Attention-BiLSTM的实现：

```python
from attention import Attention
# early stopping
from tensorflow.keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val loss', patience=10, mode='auto')
x = Input(shape=(1, x train.shape[-1]))
# LSTM

lstm_structure = Bidirectional(LSTM(64, return_sequences=True))(x)

lstm structure = Reshape((1, lstm structure.shape[-1]))(lstm structure)
lstm structure = Attention()(lstm structure)
lstm structure = Dense(128, activation='relu')(lstm structure)
lstm structure = Dense(64, activation='relu')(lstm structure)
lstm structure = Dense(32, activation='relu')(lstm structure)
# flatten
lstm structure = Flatten()(lstm structure)
lstm structure = Dense(128, activation='relu')(lstm structure)
lstm structure = Dense(64, activation='relu')(lstm structure)
lstm structure = Dense(32, activation='relu')(lstm structure)
# 输出层
output = Dense(y train.shape[-1])(lstm structure)
# 建模对象
model = Model(x, output)
# 编译
model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])
# 训练
history=model.fit(x train, y train, epochs=9999, batch size=64, validation split=0.2,
callbacks=[es])
``` 